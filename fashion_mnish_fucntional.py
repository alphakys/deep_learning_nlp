import os

from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import numpy as np
from numpy import ndarray

from keras.datasets import fashion_mnist
from keras.layers import Flatten, Dense, Layer
from keras.models import Sequential
from keras.optimizers import Adam
from keras.utils import to_categorical
from keras import Input, Model

from sklearn.model_selection import train_test_split

# í•™ìŠµì‹œí‚¬(train) ë°ì´í„°ì™€ ê²€ì¦ìš©(test) ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']


# ê° pixelì„ 255ë¡œ ë‚˜ëˆ ì„œ 0 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
# ì´ë¯¸ì§€ ì²˜ë¦¬í•  ë•Œ, íš¨ìœ¨ì´ ì˜¬ë¼ê°„ë‹¤ê³  í•œë‹¤.
def pre_process_data(images, labels):
    processed_images = np.array(images / 255.0, dtype=np.float32)
    processed_labels = np.array(labels, dtype=np.float32)
    return processed_images, processed_labels


train_images, train_labels = pre_process_data(train_images, train_labels)
test_images, test_labels = pre_process_data(test_images, test_labels)
# train datasetì„ ë‹¤ì‹œ ì´ datasetì„ ê²€ì¦í•  ìˆ˜ ìˆëŠ” validation dataset ê¹Œì§€ ì¶”ê°€ë¡œ splití•œë‹¤.
tr_images, val_images, tr_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.15,
                                                                random_state=2023)

# stringì¸ columnì„ binary í˜•íƒœë¡œ(ì¦‰ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ) encodingí•œë‹¤.
# ì¦‰ ascii ì½”ë“œë¥¼ ìƒê°í•˜ë©´ ëœë‹¤. ì•ŒíŒŒë²³ì€ ì‚¬ì‹¤ ì»´í“¨í„° ì…ì¥ì—ì„œëŠ” ìˆ«ìë¥¼ ì´ì§„ìˆ˜ í˜•íƒœë¡œ ë³€í™˜í•œ ê²ƒì„ ì¸ì‹í•˜ëŠ” ê²ƒì— ë¶ˆê³¼í•˜ë“¯ì´
tr_oh_labels = to_categorical(tr_labels)
val_oh_labels = to_categorical(val_labels)

INPUT_SIZE = 28


def create_model(input_size):
    # [STUDY] INPUTì˜ shapeì„ ë§Œë“¤ ë•Œ, ì²˜ë¦¬í•˜ê³ ì í•˜ëŠ” ë°ì´í„° í•œ í–‰ë ¬ì„ ê¸°ì¤€ìœ¼ë¡œ ë§Œë“ ë‹¤.
    #   ì°¨ì›ì´ ì•„ë‹ˆë¼
    #   ì¦‰ fasion_mnist ëª¨ë¸ì„ ì˜ˆë¡œë“ ë‹¤ë©´ 60000ê°œì˜ ì°¨ì›(6ë§Œê°œì˜ ë°ì´í„°ì„¸íŠ¸)ì´ ìˆê³  ê°ê°ì´ 28 by 28ì˜ í–‰ë ¬ì´ë‹¤.
    #   ì—¬ê¸°ì„œ 28 by 28ì˜ í–‰ë ¬ì„ shapeì— ë„£ì–´ì¤˜ì•¼í•œë‹¤.

    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))
    # flatten í•´ì¤„ ë•Œ, input ê°’ìœ¼ë¡œ call argumentì— ë„£ì–´ì¤€ë‹¤.
    # flattenì„ í•˜ë©´ 1
    x = Flatten()(input_tensor)
    x = Dense(units=100, activation='relu')(x)
    x = Dense(units=30, activation='relu')(x)
    output = Dense(units=10, activation='softmax')(x)
    model = Model(inputs=input_tensor, outputs=output, name='alpha_practice', )
    return model


model = create_model(INPUT_SIZE)
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# [STUDY] ModelCheckpoint(filepath, monitor=â€˜val_lossâ€™, verbose=0, save_best_only=False, save_weights_only=False, mode=â€˜autoâ€™, period=1)
#   íŠ¹ì • ì¡°ê±´ì— ë§ì¶°ì„œ ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥
#   filepath: filepathëŠ” (on_epoch_endì—ì„œ ì „ë‹¬ë˜ëŠ”) epochì˜ ê°’ê³¼ logsì˜ í‚¤ë¡œ ì±„ì›Œì§„ ì´ë¦„ í˜•ì‹ ì˜µì…˜ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ. ì˜ˆë¥¼ ë“¤ì–´ filepathê°€ weights.{epoch:02d}-{val_loss:.2f}.hdf5ë¼ë©´,
#   íŒŒì¼ ì´ë¦„ì— ì„¸ëŒ€ ë²ˆí˜¸ì™€ ê²€ì¦ ì†ì‹¤ì„ ë„£ì–´ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥
#   monitor: ëª¨ë‹ˆí„°í•  ì§€í‘œ(loss ë˜ëŠ” í‰ê°€ ì§€í‘œ)
#   save_best_only: ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ë§Œ ì €ì¥í•  ì—¬ë¶€
#   save_weights_only: Weightsë§Œ ì €ì¥í•  ì§€ ì—¬ë¶€
#   mode: {auto, min, max} ì¤‘ í•˜ë‚˜. monitor ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ì¢‹ì„ ê²½ìš° min, ì¦ê°€í•´ì•¼ ì¢‹ì„ ê²½ìš° max, autoëŠ” monitor ì´ë¦„ì—ì„œ ìë™ìœ¼ë¡œ ìœ ì¶”.
#   model.fit(x=tr_images, y=tr_oh_labels, epochs=20, batch_size=32, verbose=1)

mcp_cb = ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss: 2f}.hdf5', monitor='val_loss', save_best_only=True,
                         mode='min',
                         period=5, verbose=1, save_weights_only=True, model='model.h5')

# [STUDY] ReduceLROnPlateau(monitor=â€˜val_lossâ€™, factor=0.1, patience=10, verbose=0, mode=â€˜autoâ€™, min_delta=0.0001, cooldown=0, min_lr=0)
#   íŠ¹ì • epochs íšŸìˆ˜ë™ì•ˆ ì„±ëŠ¥ì´ ê°œì„  ë˜ì§€ ì•Šì„ ì‹œ Learning rateë¥¼ ë™ì ìœ¼ë¡œ ê°ì†Œ ì‹œí‚´
#   monitor: ëª¨ë‹ˆí„°í•  ì§€í‘œ(loss ë˜ëŠ” í‰ê°€ ì§€í‘œ)
#   factor: í•™ìŠµ ì†ë„ë¥¼ ì¤„ì¼ ì¸ìˆ˜. new_lr = lr * factor
#   patience: Learing Rateë¥¼ ì¤„ì´ê¸° ì „ì— monitorí•  epochs íšŸìˆ˜.
#   mode: {auto, min, max} ì¤‘ í•˜ë‚˜. monitor ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ì¢‹ì„ ê²½ìš° min, ì¦ê°€í•´ì•¼ ì¢‹ì„ ê²½ìš° max, autoëŠ” monitor ì´ë¦„ì—ì„œ ìœ ì¶”.

rlp_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min', verbose=1)

# [STUDY] EarlyStopping(monitor=â€˜val_lossâ€™, min_delta=0, patience=0, verbose=0, mode=â€˜autoâ€™, baseline=None, restore_best_weights=False)
#   íŠ¹ì • epochs ë™ì•ˆ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì„ ì‹œ í•™ìŠµì„ ì¡°ê¸°ì— ì¤‘ë‹¨
#   monitor: ëª¨ë‹ˆí„°í•  ì§€í‘œ(loss ë˜ëŠ” í‰ê°€ ì§€í‘œ)
#   patience: Early Stopping ì ìš© ì „ì— monitorí•  epochs íšŸìˆ˜.
#   mode: {auto, min, max} ì¤‘ í•˜ë‚˜. monitor ì§€í‘œê°€ ê°ì†Œí•´ì•¼ ì¢‹ì„ ê²½ìš° min, ì¦ê°€í•´ì•¼ ì¢‹ì„ ê²½ìš° max, autoëŠ” monitor ì´ë¦„ì—ì„œ ìœ ì¶”.

est_cb = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)
history = model.fit(x=tr_images, y=tr_oh_labels, batch_size=50, validation_data=(val_images, val_oh_labels),
                    callbacks=[mcp_cb, rlp_cb, est_cb],
                    epochs=40)



















#
# # functoinal api practice
# INPUT_SIZE = 28
#
# model = Sequential([
#     Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)),
#     Dense(100, activation='relu'),
#     Dense(30, activation='relu'),
#     Dense(10, activation='softmax')
# ])
#
# model.summary()
#
# model1 = Sequential()
# model1.add(Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)))
# model1.add(Dense(100, activation='relu'))
# model1.add(Dense(30, activation='relu'))
# model1.add(Dense(10, activation='softmax'))
#
#
# model1.summary()
#
# print('ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰ğŸ²ğŸ‰')
#
# # functional apiì™€ sequential ì‚¬ì´ì˜ ì°¨ì´ì ì€ input layerë¥¼ ì„¤ì •í•œë‹¤ëŠ” ì ì´ë‹¤.
# input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))
# # flattenì— ìƒì„±ì¸ìëŠ” init argumentì— ì…ë ¥ê°’ì€ call argumentì— ë„£ì–´ì¤€ë‹¤.
# x = Flatten()(input_tensor)
# x = Dense(100, activation='relu', name='Dense1')(x)
# x = Dense(30, activation='relu')(x)
# output = Dense(10, activation='softmax')(x)
#
# model2 = Model(input_tensor, output)
#
# model2.summary()
#
# import tensorflow as tf
#
# class CustomDense(Layer):
#     # ìƒì„±ìì—ì„œ class ìƒì„±ì— í•µì‹¬ì ì¸ argumentë¥¼ ë°›ëŠ”ë‹¤.
#     def __init__(self, units=32):
#         # unit defaultëŠ” 32ê°œ
#         super(CustomDense, self).__init__()
#         self.units = units
#
#     def build(self, input_shape):
#         self.w = self.add_weight(
#             name='custom_practice',
#             shape=(input_shape[-1], self.units),
#             initializer='random_normal',
#             trainable=True
#         )
#         self.b = self.add_weight(
#             shape=(self.units,), initializer='random_normal', trainable=True
#         )
#
#     # innstanceê°€ ìƒì„±ëœ í›„, call functionì—ì„œ input argumentë¥¼ ë°›ê³  outputì„ returní•œë‹¤.
#     def call(self, inputs):
#         return tf.matmul(inputs, self.w) + self.b
